{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install rake_nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnEoaNzEwINy",
        "outputId": "47fc32f3-1ab3-4fc4-d656-a645284bce78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rake_nltk\n",
            "  Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /usr/local/lib/python3.10/dist-packages (from rake_nltk) (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.6.2->rake_nltk) (4.66.4)\n",
            "Installing collected packages: rake_nltk\n",
            "Successfully installed rake_nltk-1.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t75DDurXvwx3"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import wordnet\n",
        "from rake_nltk import Rake\n",
        "\n",
        "# Download NLTK resources (if not already downloaded)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNEfCaoMv1Pq",
        "outputId": "5efa333a-fa0f-4ded-e7e4-312d30907e0e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n"
      ],
      "metadata": {
        "id": "GLJVEA_YJTXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " def get_synonyms(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "            # Adding hypernyms and hyponyms as well\n",
        "            for hyper in syn.hypernyms():\n",
        "                for lemma_hyper in hyper.lemmas():\n",
        "                    synonyms.add(lemma_hyper.name())\n",
        "            for hypo in syn.hyponyms():\n",
        "                for lemma_hypo in hypo.lemmas():\n",
        "                    synonyms.add(lemma_hypo.name())\n",
        "    # Add the original word itself\n",
        "    synonyms.add(word)\n",
        "    return synonyms\n",
        "\n",
        "def find_keywords_in_text(keyword_scores, text):\n",
        "    found_keywords = []\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [token.lower() for token in tokens]  # Convert tokens to lowercase\n",
        "    print(f\"Tokens in text2: {tokens}\")\n",
        "\n",
        "    # Generate n-grams for the entire range of the tokens\n",
        "    all_ngrams = []\n",
        "    for n in range(1, len(tokens) + 1):\n",
        "        ngrams_list = list(ngrams(tokens, n))\n",
        "        for ngram in ngrams_list:\n",
        "            all_ngrams.append(' '.join(ngram))\n",
        "\n",
        "    print(f\"Generated n-grams: {all_ngrams}\")\n",
        "\n",
        "    # Check each keyword in text\n",
        "    for keyword, score in keyword_scores:\n",
        "        keyword = keyword.lower()  # Normalize keyword to lowercase\n",
        "        stem = stemmer.stem(keyword)\n",
        "        synonyms = get_synonyms(keyword)\n",
        "        print(f\"\\nChecking keyword: '{keyword}' with score: {score}\")\n",
        "        print(f\"Stem: '{stem}', Synonyms: {synonyms}\")\n",
        "\n",
        "        # Check if any n-gram matches the keyword or any synonym\n",
        "        for ngram in all_ngrams:\n",
        "            if ngram == keyword or ngram in synonyms:\n",
        "                found_keywords.append((keyword, score))\n",
        "                print(f\"Match found: N-gram: '{ngram}'\")\n",
        "                break  # Break the loop if a match is found to avoid counting the same keyword multiple times\n",
        "\n",
        "    return found_keywords\n"
      ],
      "metadata": {
        "id": "p7REXeWFv5EM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text1 = \"\"\"\n",
        "He understood the course\n",
        "\"\"\"\n",
        "\n",
        "text2 = \"He got to comprehend the course.\"\n",
        "\n"
      ],
      "metadata": {
        "id": "XFali3tTv9S0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " r = Rake()\n",
        "r.extract_keywords_from_text(text1)\n",
        "key_words_with_scores = r.get_ranked_phrases_with_scores()\n",
        "\n",
        "# Store keywords and their scores\n",
        "keyword_scores = []\n",
        "for score, word in key_words_with_scores:\n",
        "    keyword_scores.append((word.lower(), score))\n",
        "\n",
        "# Find keywords in text2\n",
        "found_keywords = find_keywords_in_text(keyword_scores, text2)\n",
        "\n",
        "# Calculate sum of scores of found keywords in text2\n",
        "sum_found_scores = sum(score for keyword, score in found_keywords)\n",
        "\n",
        "# Calculate sum of all scores from text1\n",
        "sum_all_scores = sum(score for _, score in keyword_scores)\n",
        "\n",
        "# Calculate the ratio\n",
        "if sum_all_scores > 0:\n",
        "    ratio = sum_found_scores / sum_all_scores\n",
        "else:\n",
        "    ratio = 0.0  # Handle division by zero scenario if no keywords extracted from text1\n",
        "\n",
        "# Print results\n",
        "if found_keywords:\n",
        "    print(\"\\nKeywords found in text2:\")\n",
        "    for keyword, score in found_keywords:\n",
        "        print(f\"Keyword: '{keyword}', Score: {score}\")\n",
        "    print(f\"Sum of scores of found keywords in text2: {sum_found_scores}\")\n",
        "    print(f\"Sum of all scores from text1: {sum_all_scores}\")\n",
        "    print(f\"Ratio of sum_found_scores to sum_all_scores: {ratio}\")\n",
        "else:\n",
        "    print(\"No keywords found in text2.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ocq8T9KewBlr",
        "outputId": "35d0ef6f-8004-4d01-afe0-5e83cfdc7b97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens in text2: ['he', 'got', 'to', 'comprehend', 'the', 'course', '.']\n",
            "Generated n-grams: ['he', 'got', 'to', 'comprehend', 'the', 'course', '.', 'he got', 'got to', 'to comprehend', 'comprehend the', 'the course', 'course .', 'he got to', 'got to comprehend', 'to comprehend the', 'comprehend the course', 'the course .', 'he got to comprehend', 'got to comprehend the', 'to comprehend the course', 'comprehend the course .', 'he got to comprehend the', 'got to comprehend the course', 'to comprehend the course .', 'he got to comprehend the course', 'got to comprehend the course .', 'he got to comprehend the course .']\n",
            "\n",
            "Checking keyword: 'understood' with score: 1.0\n",
            "Stem: 'understood', Synonyms: {'interpret', 'catch', 'take_account', 'realize', 'translate', 'believe', 'comprehend', 'empathise', 'dig', 'sympathise', 'tacit', 'understand', 'compass', 'bottom', 'solve', 'get', 'sympathize', 'realise', 'get_the_picture', 'work', 'empathize', 'infer', 'silent', 'sense', 'make_out', 'perceive', 'understood', 'puzzle_out', 'lick', 'grasp', 'penetrate', 'construe', 'apprehend', 'figure_out', 'follow', 'read', 'grok', 'work_out', 'savvy', 'appreciate', 'touch', 'see', 'fathom'}\n",
            "Match found: N-gram: 'comprehend'\n",
            "\n",
            "Checking keyword: 'course' with score: 1.0\n",
            "Stem: 'cours', Synonyms: {'didactics', 'round', 'hunt_down', 'afters', 'circulate', 'current', 'jet', 'home_study', 'industrial_arts', 'sweet', 'seep', 'surge', 'collision_course', 'aliment', 'required_course', 'appetizer', 'pass_over', 'eddy', 'whirlpool', 'flush', 'action', 'course_of_study', 'nutriment', 'dessert', 'cut_across', 'extension_course', 'gutter', 'education', 'run_out', 'art_class', 'blind_alley', 'course_of_instruction', 'orientation', 'victuals', 'links_course', 'cut_through', 'traverse', 'line', 'cover', 'hunt', 'way_of_life', 'trickle', 'assemblage', 'series', 'direction', 'master_class', 'waste', 'sustenance', 'pour', 'dribble', 'raceway', 'run', 'racecourse', 'gush', 'main_course', 'belt', 'trail', 'educational_activity', 'track', 'layer', 'form', 'shop', 'spill', 'course_of_action', 'propaedeutics', 'damp_course', 'swirl', 'nutrition', 'swath', 'drain', 'entree', 'inside_track', 'naturally', 'row', 'tide', 'stream', 'discussion_section', 'filter', 'instruction', 'elective_course', 'course_of_lectures', 'gathering', 'feed', 'ooze', 'installation', 'way', 'track_down', 'row_of_bricks', 'class', 'flow', 'pedagogy', 'bed', 'move', 'facility', 'cross', 'get_across', 'purl', 'of_course', 'workshop', 'whirl', 'orientation_course', 'starter', 'steps', 'shop_class', 'get_over', 'run_down', 'childbirth-preparation_class', 'racetrack', 'well_out', 'course', 'path', 'golf_course', 'run_off', 'seminar', 'adult_education', 'directed_study', 'elective', 'correspondence_course', 'alimentation', 'appetiser', 'section', 'nourishment', 'damp-proof_course', 'propaedeutic', 'refresher', 'teaching', 'trend', 'grade', 'refresher_course'}\n",
            "Match found: N-gram: 'course'\n",
            "\n",
            "Keywords found in text2:\n",
            "Keyword: 'understood', Score: 1.0\n",
            "Keyword: 'course', Score: 1.0\n",
            "Sum of scores of found keywords in text2: 2.0\n",
            "Sum of all scores from text1: 2.0\n",
            "Ratio of sum_found_scores to sum_all_scores: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "56SDAC6n_H0b"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}